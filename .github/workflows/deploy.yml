name: Snowflake Infrastructure Deploy

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  SNOWFLAKE_ACCOUNT: "TMJESFF-ED32433"
  SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
  SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
  SNOWFLAKE_ROLE: "ACCOUNTADMIN"
  SNOWFLAKE_WAREHOUSE: "COMPUTE_WH"
  S3_PATH: "s3://bucket-for-snowflake-projects/cosmetics_etl_project"

jobs:
  check_changes:
    runs-on: ubuntu-latest
    outputs:
      infra_sql: ${{ steps.filter.outputs.infra_sql }}
      infra_all: ${{ steps.filter.outputs.infra_all }}
      pipelines: ${{ steps.filter.outputs.pipelines }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            infra_sql:
              - 'infrastructure/setup_infra.sql'
            infra_all:
              - 'infrastructure/**'
            pipelines:
              - 'pipelines/**'
              - 'orchestration/**'

  deploy:
    runs-on: ubuntu-latest
    needs: check_changes
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # --- 1. Terraform stage ---
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init & Apply
        env:
          SNOWFLAKE_ORGANIZATION_NAME: "TMJESFF"
          SNOWFLAKE_ACCOUNT_NAME:      "ED32433"
        run: |
          cd terraform
          terraform init
          terraform import snowflake_database.cosmetics_db COSMETICS_DB_DEV || true
          terraform import snowflake_schema.cosmetics_schema "\"COSMETICS_DB_DEV\".\"COSMETICS\"" || true
          terraform apply -auto-approve

      # --- 2. Python environment setup ---
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          pip install snowflake-snowpark-python pandas
          pip install great-expectations==1.8.0

      # --- 3. Execute physical infrastructure SQL (Storage Integration, External Stage, etc.) ---
      - name: Run Physical Infrastructure SQL
        if: needs.check_changes.outputs.infra_sql == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          python -c "
          import os, re
          import snowflake.snowpark as snowpark
          session = snowpark.Session.builder.configs({
              'account': '${{ env.SNOWFLAKE_ACCOUNT }}',
              'user': '${{ env.SNOWFLAKE_USER }}',
              'password': '${{ env.SNOWFLAKE_PASSWORD }}',
              'role': '${{ env.SNOWFLAKE_ROLE }}'
          }).create()
          
          def is_valid_sql(sql):
              content = re.sub(r'--.*', '', sql)
              content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
              return len(content.strip()) > 0

          try:
              sql_file = 'infrastructure/setup_infra.sql'
              print(f'üöÄ Executing physical infrastructure deployment: {sql_file}')
              with open(sql_file, 'r') as f:
                  queries = [q.strip() for q in f.read().split(';') if q.strip()]
                  for query in queries:
                      if is_valid_sql(query):
                          session.sql(query).collect()
              print('‚úÖ Physical infrastructure SQL executed successfully!')
          except Exception as e:
              print(f'‚ùå Execution failed: {str(e)}')
              exit(1)
          finally:
              session.close()
          "

      # --- 4. Run Table & Stream Setup (logical layer table structure) ---
      - name: Run Table & Stream Setup
        if: needs.check_changes.outputs.infra_all == 'true' || github.event_name == 'workflow_dispatch'
        env:
          DEPLOY_ENV: "DEV"
        run: python infrastructure/setup_tables.py

      # --- 5. Configure AWS credentials ---
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-1

      # --- 6. Generate and sync GX Configs (JSON rule files) ---
      - name: Generate and Sync GX Configs
        if: needs.check_changes.outputs.infra_all == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          python infrastructure/setup_gx.py
          aws s3 sync /tmp/gx_configs ${{ env.S3_PATH }}/gx_configs/great_expectations/ --delete

      # --- 7. Sync Python business logic code ---
      - name: Sync Pipeline Code to S3
        if: needs.check_changes.outputs.pipelines == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          aws s3 sync ./pipelines ${{ env.S3_PATH }}/python_code/ --delete

      # --- 8. Register stored procedures and start Tasks (DAG orchestration) ---
      - name: Deploy Snowflake DAG
        if: needs.check_changes.outputs.pipelines == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          python -c "
          import snowflake.snowpark as snowpark
          session = snowpark.Session.builder.configs({
              'account': '${{ env.SNOWFLAKE_ACCOUNT }}',
              'user': '${{ env.SNOWFLAKE_USER }}',
              'password': '${{ env.SNOWFLAKE_PASSWORD }}',
              'role': '${{ env.SNOWFLAKE_ROLE }}',
              'warehouse': '${{ env.SNOWFLAKE_WAREHOUSE }}',
              'database': 'COSMETICS_DB_DEV',
              'schema': 'COSMETICS'
          }).create()
          try:
              print('üöÄ Deploying DAG (Procedures & Tasks)...')
              with open('orchestration/deploy_dags.sql', 'r') as f:
                  queries = [q.strip() for q in f.read().split(';') if q.strip()]
                  for query in queries:
                      if query:
                          session.sql(query).collect()
              print('‚úÖ DAG deployment and Task startup successful!')
          finally:
              session.close()
          "