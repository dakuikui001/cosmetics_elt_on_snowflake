from snowflake.snowpark import functions as F
import great_expectations_common as gec
import time
import re
import os

class Bronze():
    def __init__(self, env, session):
        self.session = session
        env_upper = env.upper()
        
        # ğŸ”´ å¯¹é½æœ€æ–°çš„æ•°æ®åº“å‘½åæ ¼å¼ (ä¾‹å¦‚: COSMETICS_DB_DEV)
        self.env_db = f"COSMETICS_DB_{env_upper}"
        
        # ğŸ”´ å¯¹é½æœ€æ–°çš„ Stage å‘½åæ ¼å¼ (ä¾‹å¦‚: STAGE_COSMETICS_DB_DEV)
        self.stage_name = f"@{self.env_db}.COSMETICS.STAGE_{self.env_db}"
        
        self.landing_path = "raw"
        # Stream çš„å…¨è·¯å¾„ï¼Œç”¨äºæœ€åçš„å¼ºåˆ¶æ¶ˆè´¹
        self.stage_stream = f"{self.env_db}.COSMETICS.TRIGGER_S3_FILE_STREAM"
        
    def _get_new_files(self, table_name, pattern):
        """è·å– Stage ä¸Šå°šæœªå…¥åº“çš„æ–°æ–‡ä»¶"""
        files_on_stage = self.session.sql(f"LIST {self.stage_name}/{self.landing_path}").collect()
        
        all_files = [f['name'].split('/')[-1] for f in files_on_stage 
                     if f['name'].split('/')[-1].startswith(pattern) and f['name'].endswith('.csv')]
        
        try:
            # æ£€æŸ¥å·²å…¥åº“çš„æ–‡ä»¶ï¼Œé˜²æ­¢é‡å¤åŠ è½½
            processed_df = self.session.table(f"{self.env_db}.COSMETICS.{table_name}").select("SOURCE_FILE").distinct()
            processed_files = {row.SOURCE_FILE for row in processed_df.to_local_iterator()}
        except Exception:
            processed_files = set()
            
        new_files = [f for f in all_files if f not in processed_files]
        return new_files

    def _force_consume_stream(self):
        """
        å¼ºåˆ¶æ¶ˆè´¹ Stream åç§»é‡ã€‚
        è§£å†³ Snowflake Stream åœ¨é‡åˆ°ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æˆ–é€»è¾‘è·³è¿‡æ—¶æ— æ³•è‡ªåŠ¨æ¨è¿›çš„é—®é¢˜ã€‚
        """
        print(f"ğŸ”„ æ­£åœ¨å¼ºåˆ¶æ¶ˆè´¹ Stream ({self.stage_stream})...")
        
        consume_sql = f"""
            INSERT INTO {self.env_db}.COSMETICS.COSMETICS_BZ (SOURCE_FILE)
            SELECT 'dummy_ignore' 
            FROM {self.stage_stream}
            WHERE 1=0
        """
        
        try:
            self.session.sql(consume_sql).collect()
            print("âœ… Stream æŒ‡é’ˆå·²æˆåŠŸç§»åŠ¨ï¼ŒçŠ¶æ€å·²é‡ç½®ã€‚")
        except Exception as e:
            print(f"âš ï¸ å¼ºåˆ¶æ¶ˆè´¹ Stream å¤±è´¥: {str(e)}")

    def _read_and_process_incremental(self, schema_str, file_pattern, table_name):
        """æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šè¯»å– CSV -> GX æ ¡éªŒ -> å†™å…¥ Iceberg è¡¨"""
        print(f"\n--- å¼€å§‹å¤„ç†è¡¨: {table_name} ---")
        
        new_files = self._get_new_files(table_name, file_pattern)
        if not new_files:
            print(f"â˜• {table_name}: æ²¡æœ‰æ£€æµ‹åˆ°æ–°æ–‡ä»¶ï¼Œæ¸…ç† Stream...")
            self._force_consume_stream()
            return

        print(f"ğŸ“‚ åŒ¹é…åˆ° {len(new_files)} ä¸ªæ–°æ–‡ä»¶: {new_files}")
        # æ„é€ æ­£åˆ™è¡¨è¾¾å¼ï¼Œä»…è¯»å–æœ¬æ¬¡æ£€æµ‹åˆ°çš„æ–°æ–‡ä»¶
        regex_pattern = f".*({'|'.join([re.escape(f) for f in new_files])}).*"

        try:
            col_definitions = [c.strip().split(' ') for c in schema_str.split(',')]
            column_projections = ", ".join([
                f"CAST(${i+1} AS {parts[1]}) AS {parts[0].upper()}" 
                for i, parts in enumerate(col_definitions)
            ])

            sql_query = f"""
                SELECT 
                    {column_projections},
                    METADATA$FILENAME AS SOURCE_PATH,
                    SPLIT_PART(METADATA$FILENAME, '/', -1) AS SOURCE_FILE,
                    CURRENT_TIMESTAMP() AS LOAD_TIME
                FROM {self.stage_name}/{self.landing_path}
                (
                  FILE_FORMAT => '{self.env_db}.COSMETICS.BZ_CSV_FORMAT', 
                  PATTERN => '{regex_pattern}'
                )
            """

            df = self.session.sql(sql_query)
            
            # è°ƒç”¨ Great Expectations æ ¡éªŒé€»è¾‘
            batch_id = int(time.time())
            # æ³¨æ„ï¼šgec å†…éƒ¨ä¼šå¤„ç†æ•°æ®åˆ†æµï¼ˆåˆæ ¼å…¥æ­£å¼è¡¨ï¼Œä¸åˆæ ¼å…¥éš”ç¦»è¡¨ï¼‰
            gec.validate_and_insert_process_batch(df=df, batch_id=batch_id, table_name=table_name)
            
            # æˆåŠŸå¤„ç†åï¼Œæ¸…ç©º Stream è®°å½•ï¼Œé˜²æ­¢ Task å¾ªç¯è§¦å‘
            self._force_consume_stream()
            print(f"ğŸš€ {table_name}: å¢é‡åŠ è½½åŠæ ¡éªŒæˆåŠŸå®Œæˆã€‚")

        except Exception as e:
            import traceback
            print(f"âŒ {table_name} å¤„ç†å¼‚å¸¸ï¼Œä¿æŒ Stream åç§»é‡ä¸å˜ä»¥å¾…é‡è¯•:")
            print(traceback.format_exc())
            
    def consume_cosmetics_bz(self):
        """åŒ–å¦†å“åŸå§‹æ•°æ® Schema å®šä¹‰"""
        schema = "Label STRING, Brand STRING, Name STRING, Price DOUBLE, Rank DOUBLE, Ingredients STRING, Combination INT, Dry INT, Normal INT, Oily INT, Sensitive INT"
        self._read_and_process_incremental(schema, "cosmetics", "COSMETICS_BZ")

    def consume(self):
        """ä¸»å…¥å£ï¼šå…ˆåŒæ­¥ GX è§„åˆ™ï¼Œå†æ‰§è¡Œæ•°æ®åŠ å·¥"""
        start = int(time.time())
        print(f"\n--- Starting Bronze Layer Processing ---")
        
        # 1. å‡†å¤‡æœ¬åœ°é…ç½®ç›®å½•
        local_dir = "/tmp/gx_configs/expectations"
        os.makedirs(local_dir, exist_ok=True)
        
        # ğŸ”´ ä¿®æ­£ï¼šåŠ¨æ€è·å–å½“å‰ç¯å¢ƒçš„å®Œæ•´ Stage è·¯å¾„
        stage_name_full = f"{self.env_db}.COSMETICS.STAGE_{self.env_db}"
        relative_path = "gx_configs/great_expectations/expectations"
        
        print(f"ğŸ“¥ æ­£åœ¨ä» S3 Stage (@{stage_name_full}) åŒæ­¥æ ¡éªŒè§„åˆ™...")
        
        try:
            # è·å– S3 ä¸Šçš„æ‰€æœ‰ JSON è§„åˆ™æ–‡ä»¶
            files_df = self.session.sql(f"LIST @{stage_name_full}/{relative_path}").collect()
            for file_info in files_df:
                full_s3_path = file_info['name'] 
                if not full_s3_path.endswith('.json'): continue
                
                pure_file_name = full_s3_path.split('/')[-1]
                snowflake_path = f"@{stage_name_full}/{relative_path}/{pure_file_name}"
                
                # å°† S3 ä¸Šçš„è§„åˆ™æ–‡ä»¶æµå¼ä¸‹è½½åˆ°å­˜å‚¨è¿‡ç¨‹çš„æœ¬åœ° /tmp ç›®å½•
                input_stream = self.session.file.get_stream(snowflake_path)
                with open(os.path.join(local_dir, pure_file_name), "wb") as f:
                    f.write(input_stream.read())
            
            # é€šçŸ¥å…¬å…±æ¨¡å—è§„åˆ™å·²å°±ç»ª
            gec.BASE_PATH = local_dir
            gec.preload_all_suites()
            print(f"âœ… æ ¡éªŒè§„åˆ™åŠ è½½å®Œæˆã€‚")
            
        except Exception as e:
            print(f"âš ï¸ åŒæ­¥è§„åˆ™å‘Šè­¦ (å¯èƒ½ S3 ä¸ºç©º): {str(e)}")

        # 2. æ‰§è¡Œä¸»ä¸šåŠ¡é€»è¾‘
        self.consume_cosmetics_bz()
        print(f"--- Completed Bronze Layer: {int(time.time()) - start} seconds ---")